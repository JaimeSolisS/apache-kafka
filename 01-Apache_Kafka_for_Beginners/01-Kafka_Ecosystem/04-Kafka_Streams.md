# Kafka Streams 

## What is Stream Processing?
> ### Data Streams are ...  
> Unbounded - No definite starting or ending  
> Often Infinite and Ever Growing   
> Sequence of data in small packets (KB)  

The first and the most important thing is to understand that the data streams are an unbounded, infinite and ever growing sequence of data that is continuously generated and sent in small sizes in order of KBs.

Here are some examples: 

- Sensors - sending data from transportation vehicles, industrial equipment, healthcare devices, and wearables.
- Log Entries - generated by mobile apps, web applications, infrastructure components. 
- Click Streams - generated by e-commerce applications, news, and media websites, video streaming, online gaming. 
- Transactions - coming from the Stock market, Credit Card, ATM machines, eCommerce orders, payments, logistics, and food deliveries.
- Data Feeds - for example social media activities, traffic activities, security and threat systems.

The list is endless because if you look around, you are going to realize that pretty much everything can be seen as a sequence of smart data packets. And these are unbounded means infinite and ever growing. Once started, they never ever end.

### Now we have the challenge to process them:

- One common approach is to collect and store them in a storage system. Then you can do two things. Query the data to get answers to a specific question. This is what we know as a `request-response approach` and usually handled through a SQLs. This approach is all about asking one question at a time and getting the answer to the question as quickly as possible.

- The second approach is to create one big job to find answers to a bunch of queries and schedule the job to run at regular intervals. This approach is all about asking a bunch of questions at once and repeating the question every hour or maybe every day. And this approach is known as `batch processing`.

The `stream processing` sits in-between these two approaches. In this approach, we ask a question once, and the system should give you the most recent version of the answer all the time. So, stream processing is a continuous process and the business reports are updated continuously based on the data available till time. As we receive more data, the reports are updated and refreshed with the new information.

However, under the hood, you might be doing the same usual operations as joining two streams, grouping your data, computing aggregates, and other ordinary things that we do in database queries and batch processing systems.

### **Real-time stream processing is not as straightforward as data integration**.

## What is Kakfa Streams?

At the most basic level, Kafka Streams is `a library` for building applications and microservices where the `input data are streamed in a Kafka topic`. So you cannot use Kafka streams if your data is not coming to a Kafka topic. The starting point for the Kafka Stream is one or more Kafka topics.

Now the most powerful feature of Kafka streams is being a simple library. So, you can create a `standard Java and Scala applications` to perform real time stream processing. And you can `deploy your applications to any machine, virtual machine, container, or on a Kubernetes cluster`.

Your application is just another typical application with inherent `parallel processing capability, fault tolerance and scalability`  which is given to you by the Kafka streams library as an out of the box capability.

## What Kafka Stream Offers? 

Here is a list of the most critical capabilities:

1. **Working with streams tables, and interoperating with them**. You can mix and match your solutions with streams and tables and you can even convert a stream to a table and vice versa.
2. It allows you to **group your streams and compute continuously updating aggregates**.
3. You can **join streams, tables, and a combination of both**.
4. You can **create and manage fault tolerant, efficient local state stores**. 
5. **Flexible Windowing capability** creating windows of different types and 
6. **Flexible Time Schematics** dealing with all the time domain complexities such as even time, processing time, latecomers, high watermark, exactly ones processing etc.
7. **Interactive Query** It also allows you to **serve other microservices** using request/response interface over and above your streams application. This feature is also known as Kafka streams interactive query.
8. **Unit Testing Tools** It gives you a set of tools for unit testing your application.
9. The library gives you an **easy to use DLS and also offers flexibility to extend and create your custom processors** beyond what is provided.
10. **Inherent fault tolerance and dynamic scalability**.
11. You can **deploy your stream processing applications in containers** and manage them in the Kubernetes Cluster.

## Kafka Streams Architecture

So the Kafka streams is all about continuously reading a stream of data from one or more Kafka topics. And then, you develop your application logic to process those streams in real-time and take necessary actions.

So assume that you created a Kafka streams application and deployed it on a single machine. Now imagine that your Kafka streams application is continuously consuming data from `two topics`, T1 and T2 with each having three partitions.

I'm not going into the functionality of your application. It might be monitoring traffic data or maybe patient vitals, continuously checking some thresholds, and sending alerts when the threshold breaks. We are trying to understand the scalability and fault tolerance of your simple and tiny application.

So you deploy your application on a single machine. Now the Kafka streams will internally create `three logical tasks` because the **maximum number of partitions** across the input topics T1 and T2 are `three`. So the Kafka stream's framework knows that we can create `three consumers` where each could be consuming from one partition in parallel.

The most exciting part is you don't have to code this thing. The framework smartly detects it and creates `three logical tasks`. The next step is to assign partitions to these tasks. In this case, the Kafka framework would allocate the partitions *evenly*. That is one partition from each topic to each task. At the end of this assignment, `every task` will have `two partitions` to process.

Now, these tasks are ready to be assigned to application threads or instances. If you configured the application to run with `two threads`, Kafka would assign `one task to each thread`. And the remaining `third task` will go `to one of these threads` because we do not have any other thread. In this case, the task distribution is not even. The thread running two task might run slow. However, all tasks would be running and ultimately all the data gets processed.

You can make your Kafka streams become a multi threaded application by simply setting the number of **Max threads**. So as of now you will have one application instance running on a single machine and sharing the workload in two parallel threads.

Now imagine we want to scale out this application. We decided to start another instance with a single thread on a different machine. A new thread T3 will be created, and one task would automatically migrate to the new third. This happens automatically and known as Task re-assignment. When the task reassignment occurs, task partitions and their corresponding local estate stores will also migrate from the existing threat to the newly added thread.

As a result, Kafka streams has effectively rebalanced the work load among instances of the application at the granularity of Kafka topic partitions. All this happens automatically and without stopping or restarting your application.

### What if we wanted to add even more instances of the same application?

Well you can do that, but they will be doing nothing. Because we have only three tasks, which is equal to the number of available input partitions to read from. Adding more instances beyond that point is an over provisioning with idle instances.

### What about fault tolerance?

WKafka streams is out of the box fault tolerance. If a task runs on a machine that fails, Kafka streams automatically restarts the task in one of the remaining running instances. As a result, failure handling is entirely transparent to the end-user, and it is taken care of by the framework itself.